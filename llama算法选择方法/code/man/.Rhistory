perfScatterPlot <-
function(metric, modelx, modely, datax, datay=datax, addCostsx=NULL, addCostsy=NULL, pargs=NULL, ...) {
if(is.null(metric)) {
stop("Need evaluation metric for plotting!")
}
if(is.null(modelx) || is.null(modely)) {
stop("Need models to plot performances!")
}
if(is.null(datax)) {
stop("Need data to plot performances!")
}
idCols = intersect(c(datax$ids, datax$extra), c(datay$ids, datay$extra))
if(length(idCols) == 0) {
stop("Cannot match up the two data frames!")
}
if(length(datax$test) > 0) {
edatax = rbind.fill(lapply(datax$test, function(x) {
datax$data[x,idCols,drop=F]
}))
} else {
edatax = datax$data[idCols]
}
if(!is.null(datax$extra)) {
edatax[datax$extra] = datax$data[datax$extra]
}
if(length(datay$test) > 0) {
edatay = rbind.fill(lapply(datay$test, function(x) {
datay$data[x,idCols,drop=F]
}))
} else {
edatay = datay$data[idCols]
}
if(!is.null(datay$extra)) {
edatay[datay$extra] = datay$data[datay$extra]
}
scoresx = cbind(edatax, data.frame(scorex=metric(datax, modelx, addCosts=addCostsx, ...)))
scoresy = cbind(edatay, data.frame(scorey=metric(datay, modely, addCosts=addCostsy, ...)))
d = merge(scoresx, scoresy, by=idCols)
ggplot(d, aes_string(x="scorex", y="scorey")) +
geom_abline(slope = 1) +
geom_point(pargs)
}
View(perfScatterPlot)
View(perfScatterPlot)
trainTest <-
function(data, trainpart = 0.6, stratify = FALSE) {
assertClass(data, "llama.data")
assertNumeric(trainpart)
if(stratify) {
stratifier = sapply(data$best, paste, collapse="-")
} else {
stratifier = rep.int(TRUE, nrow(data$data))
}
tmp = do.call(c, by(1:nrow(data$data), stratifier, function(x) {
n = length(x)
c(rep.int(1, round(n*trainpart)), rep.int(2, n-round(n*trainpart)))[sample(n, n)]
}))
parts = split(1:nrow(data$data), tmp)
newdata = data
newdata$train = list(parts[[1]])
newdata$test = list(parts[[2]])
attr(newdata, "hasSplits") = TRUE
return(newdata)
}
classify <-
function(classifier=NULL, data=NULL, pre=function(x, y=NULL) { list(features=x) }, save.models=NA, use.weights = TRUE) {
if(!testClass(classifier, "Learner") && !testList(classifier, types="Learner")) {
stop("Need classifier or list of classifiers!")
}
assertClass(data, "llama.data")
hs = attr(data, "hasSplits")
if(is.null(hs) || hs != TRUE) {
stop("Need data with train/test split!")
}
if(testClass(classifier, "Learner")) { classifier = list(classifier) }
combinator = "majority"
if(!is.null(classifier$.combine)) {
combinator = classifier$.combine
classifier = classifier[-which(names(classifier) == ".combine")]
}
totalBests = data.frame(target=factor(breakBestTies(data), levels=data$performance))
predictions = rbind.fill(parallelMap(function(i) {
trf = pre(data$data[data$train[[i]],][data$features])
tsf = pre(data$data[data$test[[i]],][data$features], trf$meta)
ids = data$data[data$test[[i]],][data$ids]
trp = data$data[data$train[[i]],][data$performance]
trw = abs(apply(trp, 1, max) - apply(trp, 1, min))
trainpredictions = list()
ensemblepredictions = list()
trainBests = data.frame(target=factor(breakBestTies(data, i), levels=data$performance))
for(j in 1:length(classifier)) {
if(hasLearnerProperties(classifier[[j]], "weights") && use.weights) {
task = makeClassifTask(id="classify", target="target", weights = trw, data=data.frame(trainBests, trf$features))
} else {
task = makeClassifTask(id="classify", target="target", data=data.frame(trainBests, trf$features))
}
if(length(unique(trainBests$target)) == 1) {
# one-class problem
model = train(constantClassifier, task = task)
} else {
model = train(classifier[[j]], task = task)
}
if(!is.na(save.models)) {
saveRDS(list(model=model, train.data=task, test.data=tsf$features), file = paste(save.models, classifier[[j]]$id, i, "rds", sep="."))
}
if(inherits(combinator, "Learner")) { # only do this if we need it
preds = predict(model, newdata=trf$features)
trainpredictions[[j]] = if(preds$predict.type == "prob") {
getPredictionProbabilities(preds, data$performance)
} else {
tmp = getPredictionResponse(preds)
rbind.fill(lapply(tmp, function(x) data.frame(t(setNames(as.numeric(x == levels(tmp)), levels(tmp))))))
}
}
preds = predict(model, newdata=tsf$features)
ensemblepredictions[[j]] = if(preds$predict.type == "prob") {
getPredictionProbabilities(preds, data$performance)
} else {
tmp = getPredictionResponse(preds)
rbind.fill(lapply(tmp, function(x) data.frame(t(setNames(as.numeric(x == levels(tmp)), levels(tmp))))))
}
}
if(inherits(combinator, "Learner")) {
if(hasLearnerProperties(combinator, "weights") && use.weights) {
task = makeClassifTask(id="classify", target="target", weights = trw, data=data.frame(trainBests, trf$features, trainpredictions))
} else {
task = makeClassifTask(id="classify", target="target", data=data.frame(trainBests, trf$features, trainpredictions))
}
if(length(unique(trainBests$target)) == 1) {
# one-class problem
combinedmodel = train(constantClassifier, task = task)
} else {
combinedmodel = train(combinator, task = task)
}
if(!is.na(save.models)) {
saveRDS(list(model=combinedmodel, train.data=task, test.data=data.frame(tsf$features, ensemblepredictions)), file = paste(save.models, combinator$id, "combined", i, "rds", sep="."))
}
preds = predict(combinedmodel, newdata=data.frame(tsf$features, ensemblepredictions))
if(preds$predict.type == "prob") {
preds = getPredictionProbabilities(preds, data$performance)
} else {
preds = getPredictionResponse(preds)
preds = rbind.fill(lapply(preds, function(x) data.frame(t(setNames(as.numeric(x == levels(preds)), levels(preds))))))
}
combinedpredictions = rbind.fill(lapply(1:nrow(preds), function(j) {
ss = preds[j,,drop=F]
ord = order(ss, decreasing = TRUE)
data.frame(ids[j,,drop=F], algorithm=names(ss)[ord], score=as.numeric(ss)[ord], iteration=i, row.names = NULL)
}))
} else {
merged = Reduce('+', ensemblepredictions)
combinedpredictions = rbind.fill(lapply(1:nrow(merged), function(j) {
ord = order(merged[j,], decreasing = TRUE)
data.frame(ids[j,,drop=F], algorithm=names(merged)[ord], score=as.numeric(merged[j,])[ord], iteration=i, row.names = NULL)
}))
}
return(combinedpredictions)
}, 1:length(data$train), level = "llama.fold"))
fs = pre(data$data[data$features])
fp = data$data[data$performance]
fw = abs(apply(fp, 1, max) - apply(fp, 1, min))
models = lapply(1:length(classifier), function(i) {
if(hasLearnerProperties(classifier[[i]], "weights") && use.weights) {
task = makeClassifTask(id="classify", target="target", weights = fw, data=data.frame(totalBests, fs$features))
} else {
task = makeClassifTask(id="classify", target="target", data=data.frame(totalBests, fs$features))
}
if(length(unique(totalBests$target)) == 1) {
# one-class problem
model = train(constantClassifier, task = task)
} else {
model = train(classifier[[i]], task = task)
}
return(model)
})
if(inherits(combinator, "Learner")) {
trainpredictions = list()
for(i in 1:length(classifier)) {
preds = predict(models[[i]], newdata=fs$features)
trainpredictions[[i]] = if(preds$predict.type == "prob") {
getPredictionProbabilities(preds, data$performance)
} else {
tmp = getPredictionResponse(preds)
rbind.fill(lapply(tmp, function(x) data.frame(t(setNames(as.numeric(x == levels(tmp)), levels(tmp))))))
}
}
if(hasLearnerProperties(combinator, "weights") && use.weights) {
task = makeClassifTask(id="classify", target="target", weights = fw, data=data.frame(totalBests, fs$features, trainpredictions))
} else {
task = makeClassifTask(id="classify", target="target", data=data.frame(totalBests, fs$features, trainpredictions))
}
if(length(unique(totalBests$target)) == 1) {
# one-class problem
combinedmodel = train(constantClassifier, task = task)
} else {
combinedmodel = train(combinator, task = task)
}
}
predictor = function(x) {
tsf = pre(x[data$features], fs$meta)
if(length(intersect(colnames(x), data$ids)) > 0) {
ids = x[data$ids]
} else {
ids = data.frame(id = 1:nrow(x)) # don't have IDs, generate them
}
ensemblepredictions = list()
for(i in 1:length(classifier)) {
preds = predict(models[[i]], newdata=tsf$features)
ensemblepredictions[[i]] = if(preds$predict.type == "prob") {
tmp = getPredictionProbabilities(preds, data$performance)
} else {
tmp = getPredictionResponse(preds)
rbind.fill(lapply(tmp, function(x) data.frame(t(setNames(as.numeric(x == levels(tmp)), levels(tmp))))))
}
}
if(inherits(combinator, "Learner")) {
preds = predict(combinedmodel, newdata=data.frame(tsf$features, ensemblepredictions))
if(preds$predict.type == "prob") {
preds = getPredictionProbabilities(preds, data$performance)
} else {
preds = getPredictionResponse(preds)
preds = rbind.fill(lapply(preds, function(x) data.frame(t(setNames(as.numeric(x == levels(preds)), levels(preds))))))
}
combinedpredictions = rbind.fill(lapply(1:nrow(preds), function(j) {
ss = preds[j,,drop=F]
ord = order(ss, decreasing = TRUE)
data.frame(ids[j,,drop=F], algorithm=names(ss)[ord], score=as.numeric(ss)[ord], iteration=i, row.names = NULL)
}))
} else {
merged = Reduce('+', ensemblepredictions)
combinedpredictions = rbind.fill(lapply(1:nrow(merged), function(j) {
ord = order(merged[j,], decreasing = TRUE)
data.frame(ids[j,,drop=F], algorithm=names(merged)[ord], score=as.numeric(merged[j,])[ord], iteration=i, row.names = NULL)
}))
}
return(combinedpredictions)
}
class(predictor) = "llama.model"
attr(predictor, "type") = "classify"
attr(predictor, "hasPredictions") = FALSE
attr(predictor, "addCosts") = TRUE
retval = list(predictions=predictions, models=models, predictor=predictor)
class(retval) = "llama.model"
attr(retval, "type") = "classify"
attr(retval, "hasPredictions") = TRUE
attr(retval, "addCosts") = TRUE
return(retval)
}
library(testthat)
library(ParamHelpers)
tuneModel <-
function(ldf, llama.fun, learner, design, metric = parscores, nfolds = 10L, quiet = FALSE) {
assertClass(ldf, "llama.data")
assertClass(llama.fun, "llama.modelFunction")
assertClass(learner, "Learner")
assertClass(design, "data.frame")
assertClass(metric, "llama.metric")
assertInteger(nfolds)
if(length(attr(ldf, "hasSplits")) == 0) {
n.outer.folds = nfolds
ldf = cvFolds(ldf, nfolds = nfolds)
} else {
n.outer.folds = length(ldf$test)
}
inner.retval = lapply(1:n.outer.folds, function(i) {
if(!quiet) message(paste("Fold ", i, "/", n.outer.folds, ":", sep = ""))
ldf2 = ldf
ldf2$data = ldf$data[ldf$train[[i]],]
ldf3 = ldf2
ldf3 = cvFolds(ldf3, nfolds = nfolds)
best.parvals = tuneLlamaModel(ldf3, llama.fun, learner, design, metric, quiet)
outer.split.ldf = ldf
outer.split.ldf$train = list(ldf$train[[i]])
outer.split.ldf$test = list(ldf$test[[i]])
learner2 = setHyperPars(learner, par.vals = best.parvals)
model = llama.fun(learner2, data = outer.split.ldf)
retval = model$predictions
retval$iteration = i
return(list(predictions = retval, parvals = best.parvals))
})
best.parvals = tuneLlamaModel(ldf, llama.fun, learner, design, metric, quiet)
learner2 = setHyperPars(learner, par.vals = best.parvals)
full.split.ldf = ldf
full.split.ldf$train = list(ldf$train[[1]])
full.split.ldf$test = list(ldf$test[[1]])
model = llama.fun(learner2, data = full.split.ldf)
predictions = rbind.fill(lapply(inner.retval, function(x) x$predictions))
parvals = lapply(inner.retval, function(x) x$parvals)
retval = list(predictions = predictions, models = model$models, predictor = model$predictor,
parvals = best.parvals, inner.parvals = parvals)
class(retval) = "llama.model"
attr(retval, "type") = attr(model, "type")
attr(retval, "hasPredictions") = TRUE
attr(retval, "addCosts") = TRUE
return(retval)
}
source('C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/R/tune.R')
.onLoad <-
function(libname, pkgname)
.onLoad <-
function(libname, pkgname)
{
.jpackage(pkgname, lib.loc = libname)
}
load("C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/data/satsolvers.rda")
load("C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/data/satsolvers.rda")
if(length(datax$test) > 0) {
edatax = rbind.fill(lapply(datax$test, function(x) {
datax$data[x,idCols,drop=F]
}))
} else {
edatax = datax$data[idCols]
}
help getMinIndex
help getMinIndex()
View(tuneModel)
View(tuneLlamaModel)
edit(satsolvers)
edit(satsolvers)
load(llama.data)
load("llama.data")
source('C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/R/classify.R')
View(perfScatterPlot)
View(perfScatterPlot)
source('C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/R/classify.R')
source('C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/R/regression.R')
source('C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/R/regression.R')
expect_equal(ss$algorithm, factor(c("b", "c")))
View(tuneModel)
View(tuneModel)
for (j in 1:length(data$performance)) {
task = makeRegrTask(id="regression", target="target", data=cbind(data.frame(target=data$data[data$train[[i]],data$performance[j]]), trf$features))
model = train(regressor, task = task)
if(!is.na(save.models)) {
#保存数据
saveRDS(list(model=model, train.data=task, test.data=tsf$features), file = paste(save.models, regressor$id, data$performance[[j]], i, "rds", sep="."))
}
if(!is.null(combine)) {
#预测训练特征对应的结果
trainpredictions[,j] = getPredictionResponse(predict(model, newdata=trf$features))
}
performancePredictions[,j] = getPredictionResponse(predict(model, newdata=tsf$features))
}
load("llama.data")
edit(satsolvers)
clearPushBack()
system("python C:/Users/lyz/Desktop/hydra-1.1-development-cae8151/scripts/portfolio_heatmap.py")
debugSource('C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/R/classifyPairs.R')
debugSource('C:/Users/lyz/Desktop/lkotthoff-llama-ca0a9a82b440/code/R/classifyPairs.R')
if(is.null(hs) || hs != TRUE) {
stop("Need data with train/test split!")
}
load(llama-package)
load("llama-package.Rd")
load("llama-package.Rd")
setwd("C:\\Users\\lyz\\Desktop\\lkotthoff-llama-ca0a9a82b440\code\\man")
setwd("C:\\Users\\lyz\\Desktop\\lkotthoff-llama-ca0a9a82b440\\code\\man")
load("llama-package.Rd")
if(Sys.getenv("RUN_EXPENSIVE") == "true") {
library(ParamHelpers)
data(satsolvers)
learner = makeLearner("classif.J48")
# parameter set for J48
ps = makeParamSet(makeIntegerParam("M", lower = 1, upper = 100))
# generate 10 random parameter sets
design = generateRandomDesign(10, ps)
# tune with respect to PAR10 score (default) with 10 outer and inner folds
# (default)
res = tuneModel(satsolvers, classify, learner, design)
}
aa =if(Sys.getenv("RUN_EXPENSIVE") == "true") {
library(ParamHelpers)
data(satsolvers)
learner = makeLearner("classif.J48")
# parameter set for J48
ps = makeParamSet(makeIntegerParam("M", lower = 1, upper = 100))
# generate 10 random parameter sets
design = generateRandomDesign(10, ps)
# tune with respect to PAR10 score (default) with 10 outer and inner folds
# (default)
res = tuneModel(satsolvers, classify, learner, design)
}
aa
aa = if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)
res = classify(classifier=makeLearner("classif.J48"), data=folds)
# the total number of successes
sum(successes(folds, res))
# predictions on the entire data set
res$predictor(satsolvers$data[satsolvers$features])
res = classify(classifier=makeLearner("classif.svm"), data=folds)
# use probabilities instead of labels
res = classify(classifier=makeLearner("classif.randomForest", predict.type = "prob"), data=folds)
# ensemble classification
rese = classify(classifier=list(makeLearner("classif.J48"),
makeLearner("classif.IBk"),
makeLearner("classif.svm")),
data=folds)
# ensemble classification with a classifier to combine predictions
rese = classify(classifier=list(makeLearner("classif.J48"),
makeLearner("classif.IBk"),
makeLearner("classif.svm"),
.combine=makeLearner("classif.J48")),
data=folds)
}
aa
sc = parseASScenario("/path/to/scenario")
